{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лекция 2  BM5    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция ранжирования bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обратного индекса есть общепринятая формула для ранжирования *Okapi best match 25* ([Okapi BM25](https://ru.wikipedia.org/wiki/Okapi_BM25)).    \n",
    "Пусть дан запрос $Q$, содержащий слова  $q_1, ... , q_n$, тогда функция BM25 даёт следующую оценку релевантности документа $D$ запросу $Q$:\n",
    "\n",
    "$$ score(D, Q) = \\sum_{i}^{n} \\text{IDF}(q_i)*\\frac{TF(q_i,D)*(k+1)}{TF(q_i,D)+k(1-b+b\\frac{l(d)}{avgdl})} $$ \n",
    "где   \n",
    ">$TF(q_i,D)$ - частота слова $q_i$ в документе $D$      \n",
    "$l(d)$ - длина документа (количество слов в нём)   \n",
    "*avgdl* — средняя длина документа в коллекции    \n",
    "$k$ и $b$ — свободные коэффициенты, обычно их выбирают как $k$=2.0 и $b$=0.75   \n",
    "$$$$\n",
    "$\\text{IDF}(q_i)$ - это модернизированная версия IDF: \n",
    "$$\\text{IDF}(q_i) = \\log\\frac{N-n(q_i)+0.5}{n(q_i)+0.5},$$\n",
    ">> где $N$ - общее количество документов в коллекции   \n",
    "$n(q_i)$ — количество документов, содержащих $q_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Задача 1__:    \n",
    "Напишите два поисковика на *BM25*. Один через подсчет метрики по формуле для каждой пары слово-документ, второй через умножение матрицы на вектор. \n",
    "\n",
    "Сравните время работы поиска на 100к запросах. В качестве корпуса возьмем \n",
    "[Quora question pairs](https://www.kaggle.com/loopdigga/quora-question-pairs-russian)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Eduard\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Eduard\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Requirement already satisfied: pymorphy2[fast] in c:\\users\\eduard\\anaconda3\\lib\\site-packages (0.8)\n",
      "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in c:\\users\\eduard\\anaconda3\\lib\\site-packages (from pymorphy2[fast]) (2.4.393442.3710985)\n",
      "Requirement already satisfied: dawg-python>=0.7 in c:\\users\\eduard\\anaconda3\\lib\\site-packages (from pymorphy2[fast]) (0.7.2)\n",
      "Requirement already satisfied: docopt>=0.6 in c:\\users\\eduard\\anaconda3\\lib\\site-packages (from pymorphy2[fast]) (0.6.2)\n",
      "Requirement already satisfied: DAWG>=0.7.3; extra == \"fast\" in c:\\users\\eduard\\anaconda3\\lib\\site-packages (from pymorphy2[fast]) (0.7.8)\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import csv\n",
    "from math import log\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "punctuation += '…—'\n",
    "!pip install pymorphy2[fast]\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "header: ['', 'question1', 'question2', 'is_duplicate']\n"
     ]
    }
   ],
   "source": [
    "with open('quora_question_pairs_rus.csv', 'r', encoding='utf-8') as f:\n",
    "    csv_reader = csv.reader(f)\n",
    "    # skip the header\n",
    "    print('header:', next(csv_reader, None))\n",
    "    queries = []\n",
    "    docs = []\n",
    "    answers = []\n",
    "    for row in csv_reader:\n",
    "        for _, query, doc, answer in csv_reader:\n",
    "            queries.append(query)\n",
    "            docs.append(doc)\n",
    "            answers.append(int(float(answer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the first query: как я могу увеличить скорость моего интернет-соединения, используя vpn\n",
      "document: как повысить скорость интернета путем взлома через dns\n",
      "answer: 0\n"
     ]
    }
   ],
   "source": [
    "print('the first query:', queries[0])\n",
    "print('document:', docs[0])\n",
    "print('answer:', answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of documents: 404287\n"
     ]
    }
   ],
   "source": [
    "print('num of documents:', len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~ global variables\n",
    "pymorphy2_analyzer = MorphAnalyzer()\n",
    "rus_stopwords = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция препроцессинга данных\n",
    "def preprocess_text(text, save_stopwords=True, mode='slow'):\n",
    "    lowered_tokens = [word.strip(punctuation) for word in word_tokenize(text.lower()) if word.strip(punctuation)]\n",
    "    if mode == 'fast':\n",
    "        return lowered_tokens\n",
    "    if save_stopwords:\n",
    "        return [pymorphy2_analyzer.normal_forms(token)[0] for token in lowered_tokens]\n",
    "    return [pymorphy2_analyzer.normal_forms(token)[0] for token in lowered_tokens if pymorphy2_analyzer.normal_forms(token)[0] not in rus_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_slow(input_docs, input_query, preprocessing_mode='fast', k=2.0, b=0.75) -> float:\n",
    "    k = 2.0\n",
    "    b = 0.75\n",
    "    N = len(input_docs)\n",
    "\n",
    "    start = time.time()\n",
    "    docs = [preprocess_text(doc, mode=preprocessing_mode) for doc in input_docs]\n",
    "    query = preprocess_text(input_query, mode=preprocessing_mode)\n",
    "    end = time.time()\n",
    "    print('time of preprocessing:', end - start)\n",
    "\n",
    "    doc_lens = np.array([len(doc) for doc in docs])\n",
    "    average_doc_length = sum(doc_lens) / N\n",
    "    num_of_docs_with_words = [len([1 for doc in docs if word in doc]) for word in query]\n",
    "    idfs = np.array([log((N - num_of_docs_with_word + 0.5) / (num_of_docs_with_word + 0.5)) for num_of_docs_with_word in num_of_docs_with_words])\n",
    "\n",
    "    scores = []\n",
    "    for doc_index, doc in enumerate(docs):\n",
    "        score = 0\n",
    "        summand = k * (1 - b + b * doc_lens[doc_index] / average_doc_length)\n",
    "        for word_index, word in enumerate(query):\n",
    "            tf = 0\n",
    "            if doc_lens[doc_index]:\n",
    "                tf = doc.count(word) / doc_lens[doc_index]\n",
    "            score += idfs[word_index] * tf * (k + 1) / (tf + summand)\n",
    "        scores.append(score)\n",
    "    return [input_docs[index] for index in np.argsort(scores)[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time of preprocessing: 11.460090398788452\n",
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = bm25_slow(docs[:50000], \"как найти остаток\", preprocessing_mode='fast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - как найти бета-тестеры\n",
      "2 - найти выключенный телефон\n",
      "3 - как найти классы эквивалентности\n",
      "4 - как мне найти спокойствие\n",
      "5 - как найти валентность марганца\n",
      "6 - как найти домен проекта\n",
      "7 - как мне найти женщину-соучредителя\n",
      "8 - как мне найти переписчика\n",
      "9 - как найти идеального соучредителя\n",
      "10 - почему трудно найти подругу?\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join([f'{i + 1} - {elem}' for i, elem in enumerate(results[:10])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_fast(input_docs, input_query, preprocessing_mode='fast', k=2.0, b=0.75) -> float:\n",
    "    N = len(input_docs)\n",
    "    doc_lens = np.array([len(input_doc.split()) for input_doc in input_docs])\n",
    "    average_doc_length = sum(doc_lens) / N\n",
    "\n",
    "    vect = TfidfVectorizer(use_idf=False)\n",
    "    input_docs.append(input_query)\n",
    "    tf_matrix = vect.fit_transform(input_docs)\n",
    "    df = pd.DataFrame(tf_matrix.toarray(), columns=vect.get_feature_names())\n",
    "    query_vect = TfidfVectorizer(use_idf=False)\n",
    "    query_tf_matrix = query_vect.fit_transform([input_query])\n",
    "    features = query_vect.get_feature_names()\n",
    "    df = df[features][:-1]\n",
    "    df = df / (df + k * np.repeat((1 - b + b * doc_lens.reshape((N, 1)) / average_doc_length), len(features), axis=1))\n",
    "    nq = df.astype(bool).sum(axis=0)\n",
    "    idfs = np.log((N - nq + 0.5) / (nq + 0.5))\n",
    "    scores = df.dot(idfs * (k + 1))\n",
    "    return [input_docs[index] for index in np.argsort(scores)[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = bm25_fast(docs[:50000], \"как найти остаток\", preprocessing_mode='fast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - как найти бета-тестеры\n",
      "2 - как мне найти переписчика\n",
      "3 - как найти валентность марганца\n",
      "4 - как мне найти спокойствие\n",
      "5 - как найти классы эквивалентности\n",
      "6 - как найти идеального соучредителя\n",
      "7 - как найти домен проекта\n",
      "8 - найти выключенный телефон\n",
      "9 - как мне найти женщину-соучредителя\n",
      "10 - как найти подругу в Интернете?\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join([f'{i + 1} - {elem}' for i, elem in enumerate(results[:10])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты отличаются, поскольку в каждой функции используется своя токенизация."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Задача 2__:    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведите 10 первых результатов и их близость по метрике BM25 по запросу **рождественские каникулы** на нашем корпусе  Quora question pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time of preprocessing: 10.383690357208252\n",
      "Wall time: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = bm25_slow(docs[:50000], \"рождественские каникулы\", preprocessing_mode='fast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - как долго проходят рождественские каникулы для университетов в Новой Зеландии\n",
      "2 - как я могу провести летние каникулы осмысленно\n",
      "3 - людям разрешено ездить на британскую территорию индийского океана на каникулы\n",
      "4 - будучи студентом-механиком, я должен научиться c языку в мои каникулы, это поможет\n",
      "5 - какие красивые песни, которые звучат как рождественские колядки, но на самом деле не\n",
      "6 - что первый студент должен научиться в летние каникулы, что будет полезно для его будущего в области химической промышленности\n",
      "7 - это почти мои летние каникулы класса 12, я слишком поздно готовлюсь к jee mains и продвинутому 2017 году\n",
      "8 - adam d angelo: что такое стратегия монетизации кворы\n",
      "9 - как я объявляю три динамических массива 2d в c\n",
      "10 - безопасно использовать\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join([f'{i + 1} - {elem}' for i, elem in enumerate(results[:10])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = bm25_fast(docs[:50000], \"рождественские каникулы\", preprocessing_mode='fast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - как долго проходят рождественские каникулы для университетов в Новой Зеландии\n",
      "2 - как я могу провести летние каникулы осмысленно\n",
      "3 - людям разрешено ездить на британскую территорию индийского океана на каникулы\n",
      "4 - будучи студентом-механиком, я должен научиться c языку в мои каникулы, это поможет\n",
      "5 - какие красивые песни, которые звучат как рождественские колядки, но на самом деле не\n",
      "6 - это почти мои летние каникулы класса 12, я слишком поздно готовлюсь к jee mains и продвинутому 2017 году\n",
      "7 - что первый студент должен научиться в летние каникулы, что будет полезно для его будущего в области химической промышленности\n",
      "8 - adam d angelo: что такое стратегия монетизации кворы\n",
      "9 - как я объявляю три динамических массива 2d в c\n",
      "10 - безопасно использовать\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join([f'{i + 1} - {elem}' for i, elem in enumerate(results[:10])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Задача 3__:    \n",
    "\n",
    "Посчитайте точность поиска при \n",
    "1. BM25, b=0.75 \n",
    "2. BM15, b=0 \n",
    "3. BM11, b=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_precision(num_of_docs=10000, b=2):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    for query, doc, answer in zip(queries[:num_of_docs], docs[:num_of_docs], answers[:num_of_docs]):\n",
    "        results = bm25_fast(docs[:num_of_docs], query, preprocessing_mode='fast', b=b)\n",
    "        if doc in results[:5]:\n",
    "            if answer:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "    return tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5282685512367491"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "count_precision(num_of_docs=1000, b=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4934640522875817"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "count_precision(num_of_docs=1000, b=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5386029411764706"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "count_precision(num_of_docs=1000, b=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
